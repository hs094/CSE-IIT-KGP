\begin{thebibliography}{99}

% Foundation Models
\bibitem{Vaswani} Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, Ł., \\& Polosukhin, I. (2017). \textit{Attention is All You Need}. In Advances in Neural Information Processing Systems, volume 30, pages 5998-6008.

\bibitem{BERT} Devlin, J., Chang, M. W., Lee, K., \& Toutanova, K. (2019). \textit{BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding}. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1, pages 4171-4186.

\bibitem{GPT} Radford, A., Narasimhan, K., Salimans, T., \& Sutskever, I. (2018). \textit{Improving Language Understanding by Generative Pre-Training}. OpenAI Technical Report.

\bibitem{BioBERT} Lee, J., Yoon, W., Kim, S., Kim, D., Kim, S., So, C. H., \& Kang, J. (2020). \textit{BioBERT: A Pre-trained Biomedical Language Representation Model for Biomedical Text Mining}. Bioinformatics, 36(4), pages 1234-1240.

\bibitem{ClinicalBERT} Huang, K., Altosaar, J., \& Ranganath, R. (2020). \textit{ClinicalBERT: Modeling Clinical Notes and Predicting Hospital Readmission}. In Proceedings of the ACM Conference on Health, Inference, and Learning, pages 118-126.

\bibitem{MedicalLLMFineTuning} Singhal, K., Azizi, S., Tu, T., Mahdavi, S. S., Wei, J., Chung, H. W., et al. (2023). \textit{Large Language Models Encode Clinical Knowledge}. Nature, 620(7972), pages 172-180.

% Fine-tuning Techniques
\bibitem{PEFT} Houlsby, N., Giurgiu, A., Jastrzebski, S., Morrone, B., De Laroussilhe, Q., Gesmundo, A., Attariyan, M., \& Gelly, S. (2019). \textit{Parameter-Efficient Transfer Learning for NLP}. In Proceedings of the 36th International Conference on Machine Learning, pages 2790-2799.

\bibitem{hu2022lora} Hu, E. J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang, L., \& Chen, W. (2022). \textit{LoRA: Low-Rank Adaptation of Large Language Models}. In Proceedings of the International Conference on Learning Representations. Retrieved from \url{https://openreview.net/forum?id=nZeVKeeFYf9}.

\bibitem{QLoRA2023} Dettmers, T., Pagnoni, A., Holtzman, A., \& Zettlemoyer, L. (2023). \textit{QLoRA: Efficient Finetuning of Quantized LLMs}. arXiv preprint arXiv:2305.14314. Retrieved from \url{https://arxiv.org/abs/2305.14314}.

\bibitem{LLMFineTuning} Xu, T., Tan, X., Tao, Y., Cheng, H., \& Xiong, C. (2023). \textit{A Survey on Efficient Fine-tuning of Large Language Models: Methods and Applications}. arXiv preprint arXiv:2303.15647. Retrieved from \url{https://arxiv.org/abs/2303.15647}.

% RAG Systems
\bibitem{RAG} Lewis, P., Perez, E., Piktus, A., Petroni, F., Karpukhin, V., Goyal, N., Küttler, H., Lewis, M., Yih, W., Rocktäschel, T., Riedel, S., \& Kiela, D. (2020). \textit{Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks}. In Advances in Neural Information Processing Systems, volume 33, pages 9459-9474.

\bibitem{Chroma} Reimers, N., \& Gurevych, I. (2019). \textit{Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks}. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 3982-3992.

\bibitem{RAGHealthcare} Zakka, A., Agrawal, P., Sharma, A., \& Nallapati, R. (2023). \textit{Retrieval-Augmented Generation for Healthcare: A Survey}. arXiv preprint arXiv:2307.15278. Retrieved from \url{https://arxiv.org/abs/2307.15278}.

\bibitem{RAGChunking} Gao, L., Ma, X., Lin, J., \& Callan, J. (2022). \textit{Precise Zero-shot Dense Retrieval without Relevance Labels}. In Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 1221-1231.

\bibitem{RAGPrompting} Liu, J., Shen, D., Zhang, Y., Dolan, B., Carin, L., \& Chen, W. (2022). \textit{What Makes Good In-Context Examples for GPT-3?} In Proceedings of DeepLearning.AI Research Symposium.

\bibitem{RAGEvaluation} Asai, A., Wu, Z., Yu, Y., Iyer, S., \& Hajishirzi, H. (2023). \textit{Self-RAG: Learning to Retrieve, Generate, and Critique through Self-Reflection}. arXiv preprint arXiv:2310.11511. Retrieved from \url{https://arxiv.org/abs/2310.11511}.

% Mobile and Backend Development
\bibitem{Flutter} Google. (2018). \textit{Flutter: Beautiful native apps in record time}. Retrieved from \url{https://flutter.dev/}.

\bibitem{Spring} Johnson, R., Hoeller, J., Donald, K., Sampaleanu, C., Harrop, R., Risberg, T., Arendsen, A., Davison, D., Kopylenko, D., Pollack, M., et al. (2009). \textit{The Spring Framework - Reference Documentation}. Interface, 21(1), pages 27-31.

% Evaluation and Healthcare Applications
\bibitem{mHealthEngagement} Marcolino, M. S., Oliveira, J. A. Q., D'Agostino, M., Ribeiro, A. L., Alkmim, M. B. M., \& Novillo-Ortiz, D. (2018). \textit{The Impact of mHealth Interventions: Systematic Review of Systematic Reviews}. JMIR mHealth and uHealth, 6(1), e23.

\bibitem{HealthcareMetrics} Weitzman, E. R., \& Kaci, L. (2020). \textit{Developing and Evaluating Conversational Agents for Health Care: Methodological Considerations}. JMIR Medical Informatics, 8(10), e20108.

% Agentic Systems
\bibitem{MultiAgentSystems} Park, J. S., O'Brien, J. C., Cai, C. J., Morris, M. R., Liang, P., \& Bernstein, M. S. (2023). \textit{Generative Agents: Interactive Simulacra of Human Behavior}. In Proceedings of the 36th Annual ACM Symposium on User Interface Software and Technology, pages 1-22.

\bibitem{AgentRoles} Xi, Z., Chen, W., Guo, H., Liang, P., \& Hashimoto, T. B. (2023). \textit{The Role of Roles in LLM-based Cooperative Multi-Agent Systems}. arXiv preprint arXiv:2310.18140. Retrieved from \url{https://arxiv.org/abs/2310.18140}.

\bibitem{DomainExpertAgents} Chen, S., Zaharia, M., \& Zou, J. (2023). \textit{Towards Expert-Level Medical Question Answering with Large Language Models}. arXiv preprint arXiv:2305.09617. Retrieved from \url{https://arxiv.org/abs/2305.09617}.

\bibitem{PatientSimulation} Kim, J., Thaker, P., Nguyen, A., Lee, S., Osborn, H. A., \& Xu, K. (2022). \textit{BioPieces: A Modular Framework for Synthetic Patient Generation}. In Proceedings of the Conference on Health, Inference, and Learning, pages 155-172.

\bibitem{ChainOfThought} Wei, J., Wang, X., Schuurmans, D., Bosma, M., Ichter, B., Xia, F., Chi, E., Le, Q., \& Zhou, D. (2022). \textit{Chain-of-Thought Prompting Elicits Reasoning in Large Language Models}. In Advances in Neural Information Processing Systems, volume 35, pages 24824-24837.

\bibitem{HealthcareDataChallenges} Thirunavukarasu, A. J., Ting, D. S. W., \& Elangovan, K. (2023). \textit{Large Language Models in Medicine: The Potentials and Pitfalls}. Nature Medicine, 29(6), pages 1338-1340.

% Behavioral Health Applications
\bibitem{BehavioralHealthAI} Rinaldi, A., Osborne, R., \& Hall, W. (2022). \textit{Artificial Intelligence for Mental Health and Mental Illnesses: An Overview}. Current Psychiatry Reports, 24(1), pages 108-118.

\bibitem{ProchaskaTobacco} Prochaska, J. J., Vogel, E. A., \& Benowitz, N. (2022). \textit{Artificial Intelligence and Mobile Technology for Smoking Cessation}. Annual Review of Public Health, 43, pages 609-626.

\end{thebibliography}