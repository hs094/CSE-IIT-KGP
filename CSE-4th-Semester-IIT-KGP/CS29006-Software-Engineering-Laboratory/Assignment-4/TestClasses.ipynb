{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FJKFMxRxgDm4"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bqVS2d8GgDnB"
      },
      "outputs": [],
      "source": [
        "from PIL import Image"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pylab inline"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LBjZt7XCjEvE",
        "outputId": "fbaed688-bf28-4863-a418-d01973b4ab4d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Populating the interactive namespace from numpy and matplotlib\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l5ByUXJOgDnC"
      },
      "outputs": [],
      "source": [
        "im1 = Image.open(r'/content/imgs/0.jpg')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "84ZM7Pi7gDnD"
      },
      "outputs": [],
      "source": [
        "im1.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NKBLQz_igDnF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "caba4f9a-5067-440e-d71f-b3b10fb762ad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(375, 500, 3)\n"
          ]
        }
      ],
      "source": [
        "image = np.array(im1)\n",
        "print(image.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kaImCJNbgDnH"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "class FlipImage(object):\n",
        "    '''\n",
        "        Flips the image.\n",
        "    '''\n",
        "\n",
        "    def __init__(self, flip_type='horizontal'):\n",
        "        '''\n",
        "            Arguments:\n",
        "            flip_type: 'horizontal' or 'vertical' Default: 'horizontal'\n",
        "        '''\n",
        "        if flip_type not in ['horizontal', 'vertical']:\n",
        "            raise ValueError('flip_type must be either horizontal or vertical')\n",
        "        self.flip_type = flip_type\n",
        "\n",
        "        \n",
        "    def __call__(self, image):\n",
        "        '''\n",
        "            Arguments:\n",
        "            image (numpy array or PIL image)\n",
        "\n",
        "            Returns:\n",
        "            image (numpy array or PIL image)\n",
        "        '''\n",
        "        if self.flip_type == 'horizontal':\n",
        "            return np.fliplr(image)\n",
        "        else:\n",
        "            return np.flipud(image)\n",
        "\n",
        "       "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j0cATh-dgDnJ"
      },
      "outputs": [],
      "source": [
        "flipped_image = FlipImage('horizontal')(image)\n",
        "Image.fromarray(flipped_image).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1qdV8Zu7gDnL"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class RotateImage(object):\n",
        "    '''\n",
        "        Rotates the image about the centre of the image.\n",
        "    '''\n",
        "\n",
        "    def __init__(self, degrees):\n",
        "        '''\n",
        "            Arguments:\n",
        "            degrees: rotation degree.\n",
        "        '''\n",
        "        self.degrees = degrees\n",
        "\n",
        "    def __call__(self, sample):\n",
        "        '''\n",
        "            Arguments:\n",
        "            image (numpy array or PIL image)\n",
        "\n",
        "            Returns:\n",
        "            image (numpy array or PIL image)\n",
        "        '''\n",
        "        image1 = Image.fromarray(sample)\n",
        "        image1 = image1.rotate(self.degrees)\n",
        "        return np.array(image1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MTa7twQhgDnN"
      },
      "outputs": [],
      "source": [
        "rotated_image = RotateImage(69)(image)\n",
        "Image.fromarray(rotated_image).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VT6gdew0gDnP"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class RescaleImage(object):\n",
        "    '''\n",
        "        Rescales the image to a given size.\n",
        "    '''\n",
        "\n",
        "    def __init__(self, output_size):\n",
        "        '''\n",
        "            Arguments:\n",
        "            output_size (tuple or int): Desired output size. If tuple, output is\n",
        "            matched to output_size. If int, smaller of image edges is matched\n",
        "            to output_size keeping aspect ratio the same.\n",
        "        '''\n",
        "        self.output_size = output_size\n",
        "\n",
        "    def __call__(self, image):\n",
        "        '''\n",
        "            Arguments:\n",
        "            image (numpy array or PIL image)\n",
        "\n",
        "            Returns:\n",
        "            image (numpy array or PIL image)\n",
        "\n",
        "            Note: You do not need to resize the bounding boxes. ONLY RESIZE THE IMAGE.\n",
        "        '''\n",
        "        if(type(self.output_size)==int):\n",
        "            h, w = image.shape[:2]\n",
        "            if(h>w):\n",
        "                new_w = self.output_size\n",
        "                new_h = int(h*self.output_size/w)\n",
        "            else:\n",
        "                new_h = self.output_size\n",
        "                new_w = int(w*self.output_size/h)\n",
        "            image1 = Image.fromarray(image)\n",
        "            image1 = image1.resize((new_w, new_h))\n",
        "            return np.array(image1)\n",
        "        else:\n",
        "            image1 = Image.fromarray(image)\n",
        "            image1 = image1.resize(self.output_size)\n",
        "            return np.array(image1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DKkZ2tcQgDnR"
      },
      "outputs": [],
      "source": [
        "resized_image = RescaleImage((100, 500))(image)\n",
        "Image.fromarray(resized_image).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MLpbO7_ZgDnS"
      },
      "outputs": [],
      "source": [
        "from PIL import Image, ImageFilter\n",
        "import numpy as np\n",
        "\n",
        "class GaussBlurImage(object):\n",
        "    '''\n",
        "        Applies Gaussian Blur on the image.\n",
        "    '''\n",
        "\n",
        "    def __init__(self, radius):\n",
        "        '''\n",
        "            Arguments:\n",
        "            radius (int): radius to blur\n",
        "        '''\n",
        "        self.radius = radius\n",
        "        \n",
        "\n",
        "    def __call__(self, image):\n",
        "        '''\n",
        "            Arguments:\n",
        "            image (numpy array or PIL Image)\n",
        "\n",
        "            Returns:\n",
        "            image (numpy array or PIL Image)\n",
        "        '''\n",
        "        image1 = Image.fromarray(image)\n",
        "        image1 = image1.filter(ImageFilter.GaussianBlur(self.radius))\n",
        "        return np.array(image1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3dU8BGizgDnU"
      },
      "outputs": [],
      "source": [
        "blurred_image = GaussBlurImage(1)(image)\n",
        "Image.fromarray(blurred_image).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JdkgMYgwgDnU"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "class CropImage(object):\n",
        "    '''\n",
        "        Performs either random cropping or center cropping.\n",
        "    '''\n",
        "\n",
        "    def __init__(self, shape, crop_type='center'):\n",
        "        '''\n",
        "            Arguments:\n",
        "            shape: output shape of the crop (h, w)\n",
        "            crop_type: center crop or random crop. Default: center\n",
        "        '''\n",
        "        self.shape = shape\n",
        "        if crop_type not in ['center', 'random']:\n",
        "            raise ValueError('crop_type must be either center or random')\n",
        "        self.crop_type = crop_type\n",
        "\n",
        "\n",
        "    def __call__(self, image):\n",
        "        '''\n",
        "            Arguments:\n",
        "            image (numpy array or PIL image)\n",
        "\n",
        "            Returns:\n",
        "            image (numpy array or PIL image)\n",
        "        '''\n",
        "        height, width = self.shape\n",
        "        if (self.shape[0] > image.shape[0]) or (self.shape[1] > image.shape[1]):\n",
        "            raise ValueError('Crop shape must be smaller than image shape')\n",
        "        if self.crop_type == 'center':\n",
        "            y = int((image.shape[0] - height) / 2)\n",
        "            x = int((image.shape[1] - width) / 2)\n",
        "        else:\n",
        "            if (image.shape[0] - height)==0:\n",
        "                y = 0\n",
        "            else:\n",
        "                y = np.random.randint(0, image.shape[0] - height)\n",
        "            if (image.shape[1] - width)==0:\n",
        "                x = 0\n",
        "            else:\n",
        "                x = np.random.randint(0, image.shape[1] - width)\n",
        "        return image[y:y + height, x:x + width]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c4oSEwFAgDnV"
      },
      "outputs": [],
      "source": [
        "cropped_image = CropImage((180, 200), 'random')(image)\n",
        "Image.fromarray(cropped_image).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b99OaIu3gDnW"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "\n",
        "class Dataset(object):\n",
        "    '''\n",
        "        A class for the dataset that will return data items as per the given index\n",
        "    '''\n",
        "\n",
        "    def __init__(self, annotation_file, transforms = None):\n",
        "        '''\n",
        "            Arguments:\n",
        "            annotation_file: path to the annotation file\n",
        "            transforms: list of transforms (class instances)\n",
        "                        For instance, [<class 'RandomCrop'>, <class 'Rotate'>]\n",
        "        '''\n",
        "        self.annotations_path = annotation_file\n",
        "        with open(annotation_file) as file:\n",
        "            list_of_annotations = [json.loads(line) for line in file]\n",
        "        self.annotations = list_of_annotations\n",
        "        self.transforms = transforms\n",
        "        \n",
        "        \n",
        "\n",
        "    def __len__(self):\n",
        "        '''\n",
        "            return the number of data points in the dataset\n",
        "        '''\n",
        "        return len(self.annotations)\n",
        "        \n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        '''\n",
        "            return the dataset element for the index: \"idx\"\n",
        "            Arguments:\n",
        "                idx: index of the data element.\n",
        "\n",
        "            Returns: A dictionary with:\n",
        "                image: image (in the form of a numpy array) (shape: (3, H, W))\n",
        "                gt_png_ann: the segmentation annotation image (in the form of a numpy array) (shape: (1, H, W))\n",
        "                gt_bboxes: N X 5 array where N is the number of bounding boxes, each \n",
        "                            consisting of [class, x1, y1, x2, y2]\n",
        "                            x1 and x2 lie between 0 and width of the image,\n",
        "                            y1 and y2 lie between 0 and height of the image.\n",
        "\n",
        "            You need to do the following, \n",
        "            1. Extract the correct annotation using the idx provided.\n",
        "            2. Read the image, png segmentation and convert it into a numpy array (wont be necessary\n",
        "                with some libraries). The shape of the arrays would be (3, H, W) and (1, H, W), respectively.\n",
        "            3. Scale the values in the arrays to be with [0, 1].\n",
        "            4. Perform the desired transformations on the image.\n",
        "            5. Return the dictionary of the transformed image and annotations as specified.\n",
        "        '''\n",
        "\n",
        "        annotation = self.annotations[idx]\n",
        "\n",
        "        path_to_dir = self.annotations_path.replace('annotations.jsonl', '')\n",
        "        image_path = path_to_dir + annotation['img_fn']\n",
        "        image = np.array(Image.open(image_path))\n",
        "\n",
        "        #Perform the desired transformations on the image.\n",
        "        if self.transforms:\n",
        "            for transform in self.transforms:\n",
        "                image = transform(image)\n",
        "        \n",
        "        #Scale the values in the arrays to be with [0, 1].\n",
        "        image = image.transpose((2, 0, 1))\n",
        "        image = image / 255.0\n",
        "\n",
        "        gt_png_ann = np.array(Image.open(path_to_dir + annotation['png_ann_fn']))\n",
        "        gt_png_ann = gt_png_ann[..., np.newaxis].transpose((2, 0, 1))\n",
        "        gt_png_ann = gt_png_ann / 255.0\n",
        "        #print(annotation['bboxes'])\n",
        "        #print(type(annotation['bboxes']))\n",
        "        gt_bboxes = []\n",
        "        for i in range(len(annotation['bboxes'])):\n",
        "          gt_bboxes.append([annotation['bboxes'][i]['category'], annotation['bboxes'][i]['bbox'][0], annotation['bboxes'][i]['bbox'][1], annotation['bboxes'][i]['bbox'][2] + annotation['bboxes'][i]['bbox'][0], annotation['bboxes'][i]['bbox'][1]+ annotation['bboxes'][i]['bbox'][3]])\n",
        "        #gt_bboxes = np.array(annotation['bboxes']['category'], annotation['bboxes']['bbox'][0], annotation['bboxes']['bbox'][1], annotation['bboxes']['bbox'][2] + annotation['bboxes']['bbox'][0], annotation['bboxes']['bbox'][1]+ annotation['bboxes']['bbox'][3])\n",
        "        gt_bboxes = np.array(gt_bboxes)\n",
        "        #print(gt_bboxes.shape)\n",
        "        #Return the dictionary of the transformed image and annotations as specified.\n",
        "        return {'image': image, 'gt_png_ann': gt_png_ann, 'gt_bboxes': gt_bboxes}\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_oognoeLgDnX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5d1e90c0-85cd-4000-f195-85db80f0431a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'bbox': [189.82, 111.18, 72.06, 67.41], 'category': 'tv', 'category_id': 72}, {'bbox': [4.19, 148.57, 150.17, 178.69], 'category': 'chair', 'category_id': 62}, {'bbox': [201.58, 198.92, 296.3, 176.08], 'category': 'couch', 'category_id': 63}, {'bbox': [0.0, 235.0, 500.0, 140.0], 'category': 'carpet', 'category_id': 101}, {'bbox': [145.0, 167.0, 153.0, 82.0], 'category': 'shelf', 'category_id': 156}, {'bbox': [0.0, 0.0, 255.0, 257.0], 'category': 'wall-concrete', 'category_id': 172}, {'bbox': [249.0, 0.0, 251.0, 341.0], 'category': 'wall-other', 'category_id': 173}, {'bbox': [4.0, 111.0, 494.0, 264.0], 'category': 'stuff-other', 'category_id': 183}]\n",
            "<class 'list'>\n",
            "(8, 5)\n",
            "(3, 200, 100)\n"
          ]
        }
      ],
      "source": [
        "data = Dataset(r'/content/annotations.jsonl', [RescaleImage((100, 500)), GaussBlurImage(1), CropImage((200, 100), 'random')])\n",
        "image = data[0]['image']\n",
        "print(image.shape)\n",
        "image = (image*255).astype(np.uint8)\n",
        "image = image.transpose((1, 2, 0))\n",
        "Image.fromarray(image).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rjee4U35gDnY"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
        "from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n",
        "\n",
        "# Class id to name mapping\n",
        "COCO_INSTANCE_CATEGORY_NAMES = [\n",
        "    '__background__', 'person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus',\n",
        "    'train', 'truck', 'boat', 'traffic light', 'fire hydrant', 'N/A', 'stop sign',\n",
        "    'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', 'cow',\n",
        "    'elephant', 'bear', 'zebra', 'giraffe', 'N/A', 'backpack', 'umbrella', 'N/A', 'N/A',\n",
        "    'handbag', 'tie', 'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball',\n",
        "    'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard', 'tennis racket',\n",
        "    'bottle', 'N/A', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl',\n",
        "    'banana', 'apple', 'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza',\n",
        "    'donut', 'cake', 'chair', 'couch', 'potted plant', 'bed', 'N/A', 'dining table',\n",
        "    'N/A', 'N/A', 'toilet', 'N/A', 'tv', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone',\n",
        "    'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'N/A', 'book',\n",
        "    'clock', 'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush'\n",
        "]\n",
        "\n",
        "# Class definition for the model\n",
        "class InstanceSegmentationModel(object):\n",
        "\t'''\n",
        "\t\tThe blackbox image segmentation model (MaskRCNN).\n",
        "\t\tGiven an image as numpy array (3, H, W), it generates the segmentation masks.\n",
        "\t'''\n",
        "\t# __init__ function\n",
        "\tdef __init__(self):\n",
        "\t\tself.model = torchvision.models.detection.maskrcnn_resnet50_fpn(pretrained=True)\n",
        "\t\tself.model.eval()\n",
        "\n",
        "\t# function for calling the mask-rcnn model\n",
        "\tdef __call__(self, input):\n",
        "\t\t'''\n",
        "\t\t\tArguments:\n",
        "\t\t\t\tinput (numpy array): A (3, H, W) array of numbers in [0, 1] representing the image.\n",
        "\n",
        "\t\t\tReturns:\n",
        "\t\t\t\tpred_boxes (list): list of bounding boxes, [[x1 y1 x2 y2], ..] where (x1, y1) are the coordinates of the top left corner \n",
        "\t\t\t\t\t\t\t\t\tand (x2, y2) are the coordinates of the bottom right corner.\n",
        "\n",
        "\t\t\t\tpred_masks (list): list of the segmentation masks for each of the objects detected.\n",
        "\n",
        "\t\t\t\tpred_class (list): list of predicted classes.\n",
        "\n",
        "\t\t\t\tpred_score (list): list of the probability (confidence) of prediction of each of the bounding boxes.\t\t\t\t\n",
        "\n",
        "\t\t\tTip:\n",
        "\t\t\t\tYou can print the outputs to get better clarity :)\n",
        "\t\t'''\n",
        "\n",
        "\t\tinput_tensor = torch.from_numpy(input)\n",
        "\t\tinput_tensor = input_tensor.type(torch.FloatTensor)\n",
        "\t\tinput_tensor = input_tensor.unsqueeze(0)\n",
        "\t\tpredictions = self.model(input_tensor)\n",
        "\t\tprint(predictions) #uncomment this if you want to know about the output structure.\n",
        "\n",
        "\t\tpred_class = [COCO_INSTANCE_CATEGORY_NAMES[i] for i in list(predictions[0]['labels'].numpy())] # Prediction classes\n",
        "\t\tpred_masks = list(predictions[0]['masks'].detach().numpy()) # Prediction masks\n",
        "\t\tpred_boxes = [[(i[0], i[1]), (i[2], i[3])] for i in list(predictions[0]['boxes'].detach().numpy())] # Bounding boxes\n",
        "\t\tpred_score = list(predictions[0]['scores'].detach().numpy()) # Prediction scores\n",
        "\t\t\n",
        "\t\treturn pred_boxes, pred_masks, pred_class, pred_score \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = InstanceSegmentationModel()"
      ],
      "metadata": {
        "id": "pTikINAvhfxs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input = data[0]['image']\n"
      ],
      "metadata": {
        "id": "HABM6i_-ht_H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pred_boxes, pred_masks, pred_class, pred_score = model(input)"
      ],
      "metadata": {
        "id": "IaJ5CLy9h-ZQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(type(pred_boxes))"
      ],
      "metadata": {
        "id": "w_M7ypo_iRKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(pred_boxes)"
      ],
      "metadata": {
        "id": "ElUI9jkHiVlY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(x1, y1), (x2, y2) = pred_boxes[0]"
      ],
      "metadata": {
        "id": "U9JziujHpEAi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "BVcIFnWlpUEm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(pred_boxes[0])"
      ],
      "metadata": {
        "id": "q1PC9xwpiZvA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(pred_masks)"
      ],
      "metadata": {
        "id": "1SLXqnQoifiF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(pred_masks[0])"
      ],
      "metadata": {
        "id": "R3G7tU5hijuE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(type(pred_masks[0]))"
      ],
      "metadata": {
        "id": "7FdOYnYqiqF5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pred_masks[0].shape"
      ],
      "metadata": {
        "id": "Rsy4jsW3ivV5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n"
      ],
      "metadata": {
        "id": "V9EL5AV2i3UU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#mask = (pred_masks[0]*255.0).astype(np.uint8)"
      ],
      "metadata": {
        "id": "hMeHOcgxjbCD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#print(mask)"
      ],
      "metadata": {
        "id": "sw8nKlBzkehD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2"
      ],
      "metadata": {
        "id": "NHOouDWNloc8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random"
      ],
      "metadata": {
        "id": "M1hkTXcLtYSV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "number_of_boxes = len(pred_boxes)\n",
        "if(number_of_boxes<=3):\n",
        "  indices = range(number_of_boxes)\n",
        "else:\n",
        "  temp = dict()\n",
        "  for i in range(number_of_boxes):\n",
        "    temp[pred_score[i]] = i\n",
        "  indices = [temp[x] for x in temp]\n",
        "  while(len(indices)>3):\n",
        "    indices.pop()\n",
        "print(indices)\n",
        "print(\"Hello\")\n",
        "for i in indices:\n",
        "  print(i)\n",
        "  color = [random.randint(0, 255) for _ in range(3)]\n",
        "  (x1, y1), (x2, y2) = pred_boxes[i]\n",
        "  name = pred_class[i]\n",
        "  confidence = pred_score[i]\n",
        "  image = cv2.rectangle(image, (x1, y1), (x2, y2), color, 1)\n",
        "  image = cv2.putText(image, '{}: {:.3f}'.format(name, confidence), (x1, y1), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 1)\n",
        "  mask = pred_masks[i][0, :, :]\n",
        "  mask = np.stack((color[0]*mask, color[1]*mask, color[2]*mask), axis=-1).astype(np.uint8)\n",
        "  image = cv2.addWeighted(mask, 0.8, image.astype(np.uint8), 1, 0)\n",
        "plt.imshow(image)\n",
        "plt.plot()"
      ],
      "metadata": {
        "id": "q1JOe9GCkhLa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(image.shape)"
      ],
      "metadata": {
        "id": "PPqWFt7ftI8v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(mask.shape)"
      ],
      "metadata": {
        "id": "ShZCNXZkmz8V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for row in mask.tolist():\n",
        "  print(\" \".join(str(x) for x in row))"
      ],
      "metadata": {
        "id": "AXlBYoLGlxx3"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "interpreter": {
      "hash": "1cfa2540a53c64e5d30012a4f2d5efa8135932ea365fa8a35cfd1bb327d65cbd"
    },
    "kernelspec": {
      "display_name": "Python 3.9.7 64-bit ('anaconda3': conda)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "orig_nbformat": 4,
    "colab": {
      "name": "TestClasses.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}