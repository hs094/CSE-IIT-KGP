{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyOqnwRhxHFkUfuXEGpiZfOe"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# Image Classification Problem [50 marks]"],"metadata":{"id":"war7GmniYgrm"}},{"cell_type":"code","source":["import os\n","import torch\n","import torchvision\n","import tarfile\n","import pandas as pd\n","import torch.nn as nn\n","import numpy as np\n","import torch.nn.functional as F\n","from sklearn.model_selection import train_test_split\n","from torchvision.datasets.utils import download_url\n","from torchvision.datasets import ImageFolder\n","from torch.utils.data import DataLoader\n","import torchvision.transforms as tt\n","from torch.utils.data import random_split\n","from torchvision.utils import make_grid\n","import matplotlib.pyplot as plt\n","import torch.optim as optim\n","from torchvision import datasets, transforms, models\n","%matplotlib inline"],"metadata":{"id":"Gkgrj_Is6kWU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HyheEJ63YmDW","executionInfo":{"status":"ok","timestamp":1708980563681,"user_tz":-330,"elapsed":28369,"user":{"displayName":"Hardik Soni","userId":"13486846505752682152"}},"outputId":"f8be8ef3-bf36-43d7-eb12-b42dbe803e02"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["path = '/content/drive/My Drive/cifar10_archive/'"],"metadata":{"id":"imoMjRtHB9A6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Preparing datasets for further using\n","# Loading all batches and concatenating them all together\n","# Plotting first 100 examples of images from 10 different classes\n","# Preprocessing loaded CIFAR-10 dataset\n","# Saving datasets into file\n","\n","\n","\"\"\"Importing library for object serialization\n","which we'll use for saving and loading serialized models\"\"\"\n","import pickle\n","\n","# Importing other standard libraries\n","import numpy as np\n","import os\n","import matplotlib.pyplot as plt\n","\n","\n","# Defining function for loading single batch of CIFAR-10 dataset\n","def single_batch_cifar10(file):\n","    # Opening file for reading in binary mode\n","    with open(file, 'rb') as f_single_batch:\n","        d_single_batch = pickle.load(f_single_batch, encoding='latin1')  # dictionary type, we use 'latin1' for python3\n","        x = d_single_batch['data']  # numpy.ndarray type, (10000, 3072)\n","        y = d_single_batch['labels']  # list type\n","        \"\"\"Initially every batch's dictionary with key 'data' has shape (10000, 3072)\n","        Where, 10000 - number of image samples\n","        3072 - three channels of image (red + green + blue)\n","        Every row contains an image 32x32 pixels with its three channels\"\"\"\n","        # Here we reshape and transpose ndarray for further use\n","        # At the same time method 'astype()' used for converting ndarray from int to float\n","        # It is used further in function 'pre_process_cifar10' as it is needed to subtract float from float\n","        # And for standard deviation as it is needed to divide float by float\n","        x = x.reshape(10000, 3, 32, 32).transpose(0, 2, 3, 1).astype('float')  # (10000, 32, 32, 3)\n","        # Making numpy array from list of labels\n","        y = np.array(y)\n","\n","        # Returning ready data\n","        return x, y\n","\n","\n","# Defining function for loading whole CIFAR-10 dataset\n","def whole_cifar10():\n","    # Defining lists for adding all batch's data all together\n","    x_collect = []\n","    y_collect = []\n","\n","    # Defining lists for loading current batch\n","    x, y = [], []\n","\n","    # Loading all 5 batches for training and appending them together\n","    for k in range(1, 6):\n","        # Preparing current filename\n","        filename = os.path.join(path, 'datasets/cifar-10-batches-py', 'data_batch_' + str(k))\n","        # Loading current batch\n","        x, y = single_batch_cifar10(filename)\n","        # Appending data from current batch to lists\n","        x_collect.append(x)\n","        y_collect.append(y)\n","\n","    # Concatenating collected data as list of lists as one list\n","    x_train = np.concatenate(x_collect)  # (50000, 32, 32, 3)\n","    y_train = np.concatenate(y_collect)  # (50000,)\n","\n","    # Releasing memory from non-needed anymore arrays\n","    del x, y\n","\n","    # Loading data for testing\n","    filename = os.path.join(path, 'datasets/cifar-10-batches-py', 'test_batch')\n","    x_test, y_test = single_batch_cifar10(filename)\n","\n","    # Returning whole CIFAR-10 data for training and testing\n","    return x_train, y_train, x_test, y_test\n","\n","\n","# Defining function for preprocessing CIFAR-10 dataset\n","def pre_process_cifar10():\n","    # Loading whole CIFAR-10 dataset\n","    x_train, y_train, x_test, y_test = whole_cifar10()\n","\n","    # Normalizing whole data by dividing /255.0\n","    x_train /= 255.0\n","    x_test /= 255.0\n","\n","    # # Preparing data for training, validation and testing\n","    # # Data for validation is taken with 1000 examples from training dataset in range from 49000 to 50000\n","    # batch_mask = list(range(40000, 50000))\n","    # x_validation = x_train[batch_mask]  # (1000, 32, 32, 3)\n","    # y_validation = y_train[batch_mask]  # (1000,)\n","    # # Data for training is taken with first 49000 examples from training dataset\n","    # batch_mask = list(range(40000))\n","    # x_train = x_train[batch_mask]  # (49000, 32, 32, 3)\n","    # y_train = y_train[batch_mask]  # (49000,)\n","    # # Data for testing is taken with first 10000 examples from testing dataset\n","    # batch_mask = list(range(10000))\n","    # x_test = x_test[batch_mask]  # (1000, 32, 32, 3)\n","    # y_test = y_test[batch_mask]  # (1000,)\n","\n","\n","\n","    # Normalizing data by subtracting mean image and dividing by standard deviation\n","    # Subtracting the dataset by mean image serves to center the data\n","    # It helps for each feature to have a similar range and gradients don't go out of control\n","    # Calculating mean image from training dataset along the rows by specifying 'axis=0'\n","    mean_image = np.mean(x_train, axis=0)  # numpy.ndarray (32, 32, 3)\n","\n","    # Calculating standard deviation from training dataset along the rows by specifying 'axis=0'\n","    std = np.std(x_train, axis=0)  # numpy.ndarray (32, 32, 3)\n","    # Saving calculated 'mean_image' and 'std' into 'pickle' file\n","    # We will use them when preprocess input data for classifying\n","    # We will need to subtract and divide input image for classifying\n","    # As we're doing now for training, validation and testing data\n","    dictionary = {'mean_image': mean_image, 'std': std}\n","    with open(path+'datasets/'+'mean_and_std.pickle', 'wb') as f_mean_std:\n","        pickle.dump(dictionary, f_mean_std)\n","\n","    # Subtracting calculated mean image from datasets\n","    x_train -= mean_image\n","    # x_validation -= mean_image\n","    x_test -= mean_image\n","\n","    # Dividing then every dataset by standard deviation\n","    x_train /= std\n","    # x_validation /= std\n","    x_test /= std\n","\n","    # Transposing every dataset to make channels come first\n","    x_train = x_train.transpose(0, 3, 1, 2)  # (49000, 3, 32, 32)\n","    # x_validation = x_validation.transpose(0, 3, 1, 2)  # (1000, 3, 32, 32)\n","    x_test = x_test.transpose(0, 3, 1, 2)  # (1000, 3, 32, 32)\n","\n","    # Returning result as dictionary\n","    d_processed = {'x_train': x_train, 'y_train': y_train, 'x_test': x_test, 'y_test': y_test}\n","\n","    # Returning dictionary\n","    return d_processed\n","\n","\n","# Preprocessing data\n","data = pre_process_cifar10()\n","for i, j in data.items():\n","    print(i + ':', j.shape)\n","\n","# # Saving loaded and preprocessed data into 'pickle' file\n","# with open(path+'datasets/'+'data.pickle', 'wb') as f:\n","#     pickle.dump(data, f)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JqSqNM_xa8pS","executionInfo":{"status":"ok","timestamp":1708980582531,"user_tz":-330,"elapsed":18852,"user":{"displayName":"Hardik Soni","userId":"13486846505752682152"}},"outputId":"20e1e981-a946-496b-aecb-55f563654fa2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["x_train: (50000, 3, 32, 32)\n","y_train: (50000,)\n","x_test: (10000, 3, 32, 32)\n","y_test: (10000,)\n"]}]},{"cell_type":"code","source":["x_train, x_valid, y_train, y_valid = train_test_split(data['x_train'], data['y_train'], test_size=0.2, stratify=data['y_train'], random_state=0)"],"metadata":{"id":"DzQYSef9_COU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(x_train.shape)\n","print(x_valid.shape)\n","print(y_train.shape)\n","print(y_valid.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HLAmeGLeApq7","executionInfo":{"status":"ok","timestamp":1708980583439,"user_tz":-330,"elapsed":6,"user":{"displayName":"Hardik Soni","userId":"13486846505752682152"}},"outputId":"fb699572-fa10-4de2-c85a-aafbb24c4ed9"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["(40000, 3, 32, 32)\n","(10000, 3, 32, 32)\n","(40000,)\n","(10000,)\n"]}]},{"cell_type":"code","source":["class_counts_train = pd.Series(y_train).value_counts()\n","class_counts_valid = pd.Series(y_valid).value_counts()\n","\n","print(\"Train set class distribution:\")\n","print(class_counts_train)\n","print(\"\\nValidation set class distribution:\")\n","print(class_counts_valid)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xfovHjqBBLCT","executionInfo":{"status":"ok","timestamp":1708980583440,"user_tz":-330,"elapsed":6,"user":{"displayName":"Hardik Soni","userId":"13486846505752682152"}},"outputId":"7da81165-95af-421f-8584-52eb3c8211be"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Train set class distribution:\n","7    4000\n","4    4000\n","1    4000\n","5    4000\n","6    4000\n","8    4000\n","9    4000\n","0    4000\n","3    4000\n","2    4000\n","dtype: int64\n","\n","Validation set class distribution:\n","4    1000\n","3    1000\n","2    1000\n","8    1000\n","5    1000\n","7    1000\n","6    1000\n","0    1000\n","9    1000\n","1    1000\n","dtype: int64\n"]}]},{"cell_type":"code","source":["# Data Augmentation Techniques"],"metadata":{"id":"o3BjvsHyYn_T"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Convert the numpy arrays to PyTorch tensors\n","x_train_tensor = torch.tensor(x_train, dtype=torch.float32)\n","y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n","x_valid_tensor = torch.tensor(x_valid, dtype=torch.float32)\n","y_valid_tensor = torch.tensor(y_valid, dtype=torch.long)"],"metadata":{"id":"OLMrsXa1FT1J"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Create datasets and data loaders\n","batch_size = 400\n","train_dataset = torch.utils.data.TensorDataset(x_train_tensor, y_train_tensor)\n","train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","valid_dataset = torch.utils.data.TensorDataset(x_valid_tensor, y_valid_tensor)\n","valid_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)"],"metadata":{"id":"Ahp3gVGXFWr8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Define data transforms\n","transform = transforms.Compose([\n","    transforms.RandomResizedCrop(size=224),\n","    transforms.RandomHorizontalFlip(),\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n","])\n","\n","# Load pretrained Resnet-50 model\n","model = models.resnet50(pretrained=True)\n","\n","# Freeze the pre-trained layers\n","for param in model.parameters():\n","    param.requires_grad = False\n","\n","# Modify the last layer for CIFAR-10\n","num_ftrs = model.fc.in_features\n","model.fc = nn.Linear(num_ftrs, 10)  # 10 classes in CIFAR-10\n","\n","# Replace the last layer with a new one for our 10 classes\n","num_classes = 10\n","model.fc = nn.Linear(model.fc.in_features, num_classes)\n","\n","# Define loss function and optimizer\n","criterion = nn.NLLLoss()\n","optimizer = optim.Adam(model.fc.parameters(), lr=0.1)\n","\n","# Train the model\n","model.train()\n","for epoch in range(5):  # Train for 5 epochs\n","    for inputs, labels in train_loader:\n","        optimizer.zero_grad()\n","        outputs = model(inputs)\n","        loss = criterion(outputs, labels)\n","        loss.backward()\n","        optimizer.step()\n","\n","# Validate the model\n","model.eval()\n","correct = 0\n","total = 0\n","with torch.no_grad():\n","    for inputs, labels in valid_loader:\n","        outputs = model(inputs)\n","        _, predicted = torch.max(outputs, 1)\n","        total += labels.size(0)\n","        correct += (predicted == labels).sum().item()\n","\n","accuracy = correct / total\n","print('Accuracy on the validation set: {:.2f}%'.format(100 * accuracy))"],"metadata":{"id":"JKTK7C95amIM","colab":{"base_uri":"https://localhost:8080/"},"outputId":"c7f80618-82d7-4b50-b39b-c0a4a29a5d3e","executionInfo":{"status":"ok","timestamp":1708981687040,"user_tz":-330,"elapsed":860689,"user":{"displayName":"Hardik Soni","userId":"13486846505752682152"}}},"execution_count":11,"outputs":[{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n","  warnings.warn(msg)\n","Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n","100%|██████████| 97.8M/97.8M [00:00<00:00, 146MB/s]\n"]},{"output_type":"stream","name":"stdout","text":["Accuracy on the validation set: 10.07%\n"]}]}]}